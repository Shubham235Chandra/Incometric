{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "# Load the dataset\n",
    "file_path = '/mnt/data/data_modified.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "print(\"Dataset head:\")\n",
    "print(data.head())\n",
    "\n",
    "# Checking for missing values\n",
    "missing_values = data.isnull().sum()\n",
    "print(\"\\nMissing values:\")\n",
    "print(missing_values)\n",
    "\n",
    "# Fill missing values or drop columns with too many missing values\n",
    "data = data.dropna()  # For simplicity, we drop rows with missing values\n",
    "\n",
    "# Handle categorical variables if necessary\n",
    "categorical_cols = data.select_dtypes(include=['object']).columns\n",
    "\n",
    "# Display the cleaned dataset\n",
    "print(\"\\nCleaned dataset head:\")\n",
    "print(data.head())\n",
    "\n",
    "# Define features and target\n",
    "X = data.drop('income', axis=1)\n",
    "y = data['income']\n",
    "\n",
    "# Identify numerical and categorical columns\n",
    "num_cols = X.select_dtypes(include=['int64', 'float64']).columns\n",
    "cat_cols = X.select_dtypes(include=['object']).columns\n",
    "\n",
    "# Create preprocessing pipelines for numerical and categorical data\n",
    "num_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "cat_pipeline = Pipeline([\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Combine pipelines into a single ColumnTransformer\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', num_pipeline, num_cols),\n",
    "    ('cat', cat_pipeline, cat_cols)\n",
    "])\n",
    "\n",
    "# Fit and transform the entire dataset\n",
    "X_processed = preprocessor.fit_transform(X)\n",
    "\n",
    "# Convert processed data back to DataFrame for feature selection\n",
    "X_processed_df = pd.DataFrame(X_processed, columns=preprocessor.get_feature_names_out())\n",
    "\n",
    "# Feature correlation analysis\n",
    "correlation_matrix = X_processed_df.corr().abs()\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')\n",
    "plt.title('Feature Correlation Matrix')\n",
    "plt.show()\n",
    "\n",
    "# Drop features with low correlation to the target (assuming a threshold, e.g., 0.1)\n",
    "threshold = 0.1\n",
    "low_correlation_features = [column for column in correlation_matrix.columns if correlation_matrix[column].mean() < threshold]\n",
    "X_processed_df.drop(columns=low_correlation_features, inplace=True)\n",
    "\n",
    "# Apply PCA for feature reduction\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=0.95)  # Retain 95% of variance\n",
    "X_pca = pca.fit_transform(X_processed_df)\n",
    "\n",
    "# Split the data after preprocessing and feature selection\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X_pca, y, test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Initialize models\n",
    "models = {\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'Ridge': Ridge(),\n",
    "    'Lasso': Lasso(),\n",
    "    'Random Forest': RandomForestRegressor(random_state=42),\n",
    "    'Gradient Boosting': GradientBoostingRegressor(random_state=42),\n",
    "    'Support Vector Regressor': SVR(),\n",
    "    'XGBoost': XGBRegressor(random_state=42),\n",
    "    'MLP Regressor': MLPRegressor(random_state=42)\n",
    "}\n",
    "\n",
    "# Train models and evaluate on the validation set\n",
    "metrics = {}\n",
    "for name, model in models.items():\n",
    "    start_time = time.time()\n",
    "    model.fit(X_train, y_train)\n",
    "    end_time = time.time()\n",
    "    y_val_pred = model.predict(X_val)\n",
    "    rmse = mean_squared_error(y_val, y_val_pred, squared=False)\n",
    "    mae = mean_absolute_error(y_val, y_val_pred)\n",
    "    r2 = r2_score(y_val, y_val_pred)\n",
    "    metrics[name] = {'RMSE': rmse, 'MAE': mae, 'R^2': r2, 'Time': end_time - start_time}\n",
    "    print(f'{name} RMSE: {rmse}, MAE: {mae}, R^2: {r2}, Time: {end_time - start_time} seconds')\n",
    "\n",
    "# Define parameter grids for hyperparameter tuning\n",
    "param_grids = {\n",
    "    'Random Forest': {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'max_depth': [None, 10, 20],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4]\n",
    "    },\n",
    "    'Gradient Boosting': {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'learning_rate': [0.01, 0.1, 0.2],\n",
    "        'max_depth': [3, 4, 5]\n",
    "    },\n",
    "    'XGBoost': {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'learning_rate': [0.01, 0.1, 0.2],\n",
    "        'max_depth': [3, 4, 5]\n",
    "    },\n",
    "    'MLP Regressor': {\n",
    "        'hidden_layer_sizes': [(50, 50), (100, 50), (100, 100)],\n",
    "        'activation': ['relu', 'tanh'],\n",
    "        'learning_rate': ['constant', 'adaptive']\n",
    "    }\n",
    "}\n",
    "\n",
    "# Function to perform Grid Search CV\n",
    "def perform_grid_search(model, param_grid, X_train, y_train):\n",
    "    grid_search = GridSearchCV(estimator=model,\n",
    "                               param_grid=param_grid,\n",
    "                               cv=3,\n",
    "                               scoring='neg_mean_squared_error',\n",
    "                               n_jobs=-1,\n",
    "                               verbose=2)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    return grid_search.best_estimator_, grid_search.best_params_, grid_search.best_score_\n",
    "\n",
    "# Perform Grid Search CV for selected models\n",
    "best_models = {}\n",
    "for model_name in ['Random Forest', 'Gradient Boosting', 'XGBoost', 'MLP Regressor']:\n",
    "    print(f\"\\nTuning {model_name}...\")\n",
    "    model = models[model_name]\n",
    "    param_grid = param_grids[model_name]\n",
    "    start_time = time.time()\n",
    "    best_model, best_params, best_score = perform_grid_search(model, param_grid, X_train, y_train)\n",
    "    end_time = time.time()\n",
    "    best_models[model_name] = best_model\n",
    "    y_val_pred = best_model.predict(X_val)\n",
    "    rmse = mean_squared_error(y_val, y_val_pred, squared=False)\n",
    "    mae = mean_absolute_error(y_val, y_val_pred)\n",
    "    r2 = r2_score(y_val, y_val_pred)\n",
    "    metrics[f'{model_name} (Tuned)'] = {'RMSE': rmse, 'MAE': mae, 'R^2': r2, 'Time': end_time - start_time}\n",
    "    print(f'Best parameters: {best_params}')\n",
    "    print(f'Best score: {best_score}')\n",
    "    print(f'{model_name} (Tuned) RMSE: {rmse}, MAE: {mae}, R^2: {r2}, Time: {end_time - start_time} seconds')\n",
    "\n",
    "# Deep Learning Model\n",
    "def build_dl_model(input_dim):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(64, input_dim=input_dim, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    return model\n",
    "\n",
    "# Build and train the deep learning model\n",
    "input_dim = X_train.shape[1]\n",
    "dl_model = build_dl_model(input_dim)\n",
    "start_time = time.time()\n",
    "dl_model.fit(X_train, y_train, epochs=100, batch_size=32, validation_data=(X_val, y_val), verbose=2)\n",
    "end_time = time.time()\n",
    "\n",
    "# Evaluate the deep learning model\n",
    "y_val_pred_dl = dl_model.predict(X_val)\n",
    "rmse_dl = mean_squared_error(y_val, y_val_pred_dl, squared=False)\n",
    "mae_dl = mean_absolute_error(y_val, y_val_pred_dl)\n",
    "r2_dl = r2_score(y_val, y_val_pred_dl)\n",
    "metrics['Deep Learning Model'] = {'RMSE': rmse_dl, 'MAE': mae_dl, 'R^2': r2_dl, 'Time': end_time - start_time}\n",
    "print(f'Deep Learning Model RMSE: {rmse_dl}, MAE: {mae_dl}, R^2: {r2_dl}, Time: {end_time - start_time} seconds')\n",
    "\n",
    "# Evaluate all models on the test set\n",
    "for name, model in best_models.items():\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    rmse = mean_squared_error(y_test, y_test_pred, squared=False)\n",
    "    mae = mean_absolute_error(y_test, y_test_pred)\n",
    "    r2 = r2_score(y_test, y_test_pred)\n",
    "    print(f'{name} Test RMSE: {rmse}, MAE: {mae}, R^2: {r2}')\n",
    "\n",
    "# Evaluate the deep learning model on the test set\n",
    "y_test_pred_dl = dl_model.predict(X_test)\n",
    "rmse_dl_test = mean_squared_error(y_test, y_test_pred_dl, squared=False)\n",
    "mae_dl_test = mean_absolute_error(y_test, y_test_pred_dl)\n",
    "r2_dl_test = r2_score(y_test, y_test_pred_dl)\n",
    "print(f'Deep Learning Model Test RMSE: {rmse_dl_test}, MAE: {mae_dl_test}, R^2: {r2_dl_test}')\n",
    "\n",
    "# Save the best model (Random Forest in this case)\n",
    "best_rf_model = best_models['Random Forest']\n",
    "with open('/mnt/data/best_model.pkl', 'wb') as file:\n",
    "    pickle.dump(best_rf_model, file)\n",
    "\n",
    "# Display the collected metrics\n",
    "import ace_tools as tools; tools.display_dataframe_to_user(name=\"Model Performance Metrics\", dataframe=pd.DataFrame(metrics).T)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
