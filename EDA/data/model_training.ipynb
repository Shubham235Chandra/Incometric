{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from tabulate import tabulate\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, MinMaxScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor, BaggingRegressor, HistGradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "file_path = 'data_modified.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "print(tabulate(data.head(), headers='keys', tablefmt='rounded_grid', showindex=\"always\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features and target\n",
    "X = data.drop('Income', axis=1)\n",
    "y = data['Income']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify numerical and categorical columns\n",
    "num_cols = X.select_dtypes(include=['int64', 'float64']).columns\n",
    "cat_cols = X.select_dtypes(include=['object']).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create preprocessing pipelines for numerical and categorical data\n",
    "num_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "cat_pipeline = Pipeline([\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Combine pipelines into a single ColumnTransformer\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', num_pipeline, num_cols),\n",
    "    ('cat', cat_pipeline, cat_cols)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit and transform the entire dataset\n",
    "X_processed = preprocessor.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data after preprocessing\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X_processed, y, test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display shapes of the datasets\n",
    "data_shapes = [\n",
    "    [\"Dataset\", \"Shape\"],\n",
    "    [\"X_train\", X_train.shape],\n",
    "    [\"y_train\", y_train.shape],\n",
    "    [\"X_val\", X_val.shape],\n",
    "    [\"y_val\", y_val.shape],\n",
    "    [\"X_test\", X_test.shape],\n",
    "    [\"y_test\", y_test.shape]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the table\n",
    "print(tabulate(data_shapes, headers=\"firstrow\", tablefmt=\"grid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize models\n",
    "models = {\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'Ridge': Ridge(),\n",
    "    'Lasso': Lasso(),\n",
    "    'ElasticNet': ElasticNet(),\n",
    "    'Random Forest': RandomForestRegressor(random_state=6112024),\n",
    "    'Gradient Boosting': GradientBoostingRegressor(random_state=6112024),\n",
    "    'Support Vector Regressor': SVR(),\n",
    "    'XGBoost': XGBRegressor(random_state=6112024),\n",
    "    'MLP Regressor': MLPRegressor(random_state=6112024),\n",
    "    'Decision Tree': DecisionTreeRegressor(random_state=6112024),\n",
    "    'AdaBoost': AdaBoostRegressor(random_state=6112024),\n",
    "    'Bagging': BaggingRegressor(random_state=6112024),\n",
    "    'Kernel Ridge': KernelRidge(),\n",
    "    'Gaussian Process': GaussianProcessRegressor(),\n",
    "    'KNeighbors': KNeighborsRegressor(),\n",
    "    'Hist Gradient Boosting': HistGradientBoostingRegressor(random_state=6112024)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameter grids for Hyperparameter Tuning\n",
    "param_grids = {\n",
    "    'Random Forest': {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'max_depth': [None, 10, 20],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4]\n",
    "    },\n",
    "    'Gradient Boosting': {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'learning_rate': [0.01, 0.1, 0.2],\n",
    "        'max_depth': [3, 4, 5]\n",
    "    },\n",
    "    'XGBoost': {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'learning_rate': [0.01, 0.1, 0.2],\n",
    "        'max_depth': [3, 4, 5]\n",
    "    },\n",
    "    'MLP Regressor': {\n",
    "        'hidden_layer_sizes': [(50, 50), (100, 50), (100, 100)],\n",
    "        'activation': ['relu', 'tanh'],\n",
    "        'learning_rate': ['constant', 'adaptive']\n",
    "    },\n",
    "    'Decision Tree': {\n",
    "        'max_depth': [None, 10, 20, 30],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4]\n",
    "    },\n",
    "    'AdaBoost': {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'learning_rate': [0.01, 0.1, 1.0]\n",
    "    },\n",
    "    'Bagging': {\n",
    "        'n_estimators': [10, 50, 100],\n",
    "        'max_samples': [0.5, 0.75, 1.0],\n",
    "        'max_features': [0.5, 0.75, 1.0]\n",
    "    },\n",
    "    'Kernel Ridge': {\n",
    "        'alpha': [0.1, 1.0, 10.0],\n",
    "        'kernel': ['linear', 'rbf', 'poly'],\n",
    "        'gamma': [0.01, 0.1, 1.0, None]\n",
    "    },\n",
    "    'Gaussian Process': {\n",
    "        'alpha': [1e-10, 1e-2, 1.0],\n",
    "        'n_restarts_optimizer': [0, 1, 2]\n",
    "    },\n",
    "    'KNeighbors': {\n",
    "        'n_neighbors': [3, 5, 10],\n",
    "        'weights': ['uniform', 'distance'],\n",
    "        'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute']\n",
    "    },\n",
    "    'Hist Gradient Boosting': {\n",
    "        'learning_rate': [0.01, 0.1, 0.2],\n",
    "        'max_iter': [100, 200, 300],\n",
    "        'max_depth': [None, 10, 20],\n",
    "        'min_samples_leaf': [20, 50, 100]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train models and evaluate on the validation set\n",
    "metrics = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    start_time = time.time()\n",
    "    model.fit(X_train, y_train)\n",
    "    end_time = time.time()\n",
    "    \n",
    "    y_val_pred = model.predict(X_val)\n",
    "    rmse = mean_squared_error(y_val, y_val_pred, squared=False)\n",
    "    mae = mean_absolute_error(y_val, y_val_pred)\n",
    "    r2 = r2_score(y_val, y_val_pred)\n",
    "    \n",
    "    metrics[name] = {\n",
    "        'RMSE': rmse,\n",
    "        'MAE': mae,\n",
    "        'R^2': r2,\n",
    "        'Time': end_time - start_time\n",
    "    }\n",
    "    \n",
    "    print(f'{name} RMSE: {rmse}, MAE: {mae}, R^2: {r2}, Time: {end_time - start_time} seconds')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for tabulation\n",
    "metrics_table = [[\"Model\", \"RMSE\", \"MAE\", \"R^2\", \"Time (seconds)\"]]\n",
    "for model_name, metric_values in metrics.items():\n",
    "    metrics_table.append([\n",
    "        model_name, \n",
    "        f\"{metric_values['RMSE']:.3f}\", \n",
    "        f\"{metric_values['MAE']:.3f}\", \n",
    "        f\"{metric_values['R^2']:.3f}\", \n",
    "        f\"{metric_values['Time']:.4f}\"\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the table\n",
    "print(tabulate(metrics_table, headers=\"firstrow\", tablefmt=\"grid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert metrics dictionary to DataFrame for easier processing\n",
    "metrics_df = pd.DataFrame(metrics).T\n",
    "\n",
    "# Normalize the metrics using min-max scaling\n",
    "scaler = MinMaxScaler()\n",
    "normalized_metrics = scaler.fit_transform(metrics_df[['RMSE', 'MAE', 'R^2']])\n",
    "normalized_df = pd.DataFrame(normalized_metrics, columns=['RMSE', 'MAE', 'R^2'], index=metrics_df.index)\n",
    "\n",
    "# Assign weights to each metric\n",
    "weights = {'RMSE': 0.4, 'MAE': 0.3, 'R^2': 0.3}\n",
    "\n",
    "# Compute the weighted score for each model\n",
    "normalized_df['Score'] = (\n",
    "    weights['RMSE'] * (1 - normalized_df['RMSE']) +  # Lower RMSE is better, hence (1 - RMSE)\n",
    "    weights['MAE'] * (1 - normalized_df['MAE']) +  # Lower MAE is better, hence (1 - MAE)\n",
    "    weights['R^2'] * normalized_df['R^2']  # Higher R^2 is better\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the best model based on the highest score\n",
    "best_model_name = normalized_df['Score'].idxmax()\n",
    "best_model_metrics = metrics[best_model_name]\n",
    "best_model_instance = models[best_model_name]\n",
    "\n",
    "# Print the best model metrics\n",
    "best_model_table = [\n",
    "    [\"Metric\", \"Value\"],\n",
    "    [\"Best Model\", best_model_name],\n",
    "    [\"RMSE\", f\"{best_model_metrics['RMSE']:.3f}\"],\n",
    "    [\"MAE\", f\"{best_model_metrics['MAE']:.3f}\"],\n",
    "    [\"R^2\", f\"{best_model_metrics['R^2']:.3f}\"],\n",
    "    [\"Time (seconds)\", f\"{best_model_metrics['Time']:.4f}\"]\n",
    "]\n",
    "print(\"\\nBest Model Metrics:\")\n",
    "print(tabulate(best_model_table, headers=\"firstrow\", tablefmt=\"grid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the predictions vs actual values for the best model\n",
    "y_test_pred = best_model_instance.predict(X_test)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.scatter(y_test, y_test_pred, alpha=0.6, edgecolors='w', linewidth=0.5, color='royalblue', label='Predictions')\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], color='red', linestyle='--', linewidth=2, label='Perfect Prediction')\n",
    "plt.xlabel('Actual Values', fontsize=14)\n",
    "plt.ylabel('Predicted Values', fontsize=14)\n",
    "plt.title(f'{best_model_name} Predictions vs Actual Values', fontsize=16)\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.legend(fontsize=12)\n",
    "plt.tick_params(axis='both', which='major', labelsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_grid_search(model, param_grid, X_train, y_train):\n",
    "    grid_search = GridSearchCV(model, param_grid, cv=5, n_jobs=-1)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    best_model = grid_search.best_estimator_\n",
    "    best_params = grid_search.best_params_\n",
    "    best_score = grid_search.best_score_\n",
    "    return best_model, best_params, best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform Grid Search CV for selected models\n",
    "metrics_param = {}\n",
    "best_models = {}\n",
    "\n",
    "for model_name in param_grids.keys():\n",
    "    print(f\"\\nTuning {model_name}...\")\n",
    "    model = models[model_name]\n",
    "    param_grid = param_grids[model_name]\n",
    "    start_time = time.time()\n",
    "    best_model, best_params, best_score = perform_grid_search(model, param_grid, X_train, y_train)\n",
    "    end_time = time.time()\n",
    "    best_models[model_name] = best_model\n",
    "    y_val_pred = best_model.predict(X_val)\n",
    "    rmse = mean_squared_error(y_val, y_val_pred, squared=False)\n",
    "    mae = mean_absolute_error(y_val, y_val_pred)\n",
    "    r2 = r2_score(y_val, y_val_pred)\n",
    "    metrics_param[f'{model_name} (Tuned)'] = {'RMSE': rmse, 'MAE': mae, 'R^2': r2, 'Time': end_time - start_time}\n",
    "    print(f'Best parameters: {best_params}')\n",
    "    print(f'Best score: {best_score}')\n",
    "    print(f'{model_name} (Tuned) RMSE: {rmse}, MAE: {mae}, R^2: {r2}, Time: {end_time - start_time} seconds')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for tabulation\n",
    "metrics_table_param = [[\"Model\", \"RMSE\", \"MAE\", \"R^2\", \"Time (seconds)\"]]\n",
    "for model_name, metric_values in metrics_param.items():\n",
    "    metrics_table_param.append([\n",
    "        model_name, \n",
    "        f\"{metric_values['RMSE']:.2f}\", \n",
    "        f\"{metric_values['MAE']:.2f}\", \n",
    "        f\"{metric_values['R^2']:.6f}\", \n",
    "        f\"{metric_values['Time']:.6f}\"\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the table\n",
    "print(tabulate(metrics_table_param, headers=\"firstrow\", tablefmt=\"grid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the metrics for models with hyperparameter tuning\n",
    "metrics_df_param = pd.DataFrame(metrics_param).T\n",
    "\n",
    "# Normalize the metrics using MinMax scaling\n",
    "scaler = MinMaxScaler()\n",
    "metrics_df_param[['RMSE', 'MAE', 'R^2']] = scaler.fit_transform(metrics_df_param[['RMSE', 'MAE', 'R^2']])\n",
    "\n",
    "# Assign weights to each metric\n",
    "weights = {'RMSE': 0.4, 'MAE': 0.3, 'R^2': 0.3}\n",
    "\n",
    "# Compute a weighted score for each model\n",
    "metrics_df_param['Score'] = (\n",
    "    weights['RMSE'] * (1 - metrics_df_param['RMSE']) +  # Lower RMSE is better, hence (1 - RMSE)\n",
    "    weights['MAE'] * (1 - metrics_df_param['MAE']) +  # Lower MAE is better, hence (1 - MAE)\n",
    "    weights['R^2'] * metrics_df_param['R^2']  # Higher R^2 is better\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the best model based on the highest score\n",
    "best_model_name_param = metrics_df_param['Score'].idxmax()\n",
    "best_model_metrics_param = metrics_param[best_model_name_param]\n",
    "best_model_instance_param = best_models[best_model_name_param.split(\" (Tuned)\")[0]]  # Remove the \"(Tuned)\" part to get the model name\n",
    "\n",
    "# Print the best model metrics\n",
    "best_model_table_param = [\n",
    "    [\"Metric\", \"Value\"],\n",
    "    [\"Best Model\", best_model_name_param],\n",
    "    [\"RMSE\", f\"{best_model_metrics_param['RMSE']:.3f}\"],\n",
    "    [\"MAE\", f\"{best_model_metrics_param['MAE']:.3f}\"],\n",
    "    [\"R^2\", f\"{best_model_metrics_param['R^2']:.6f}\"],\n",
    "    [\"Time (seconds)\", f\"{best_model_metrics_param['Time']:.6f}\"]\n",
    "]\n",
    "print(\"\\nBest Model Metrics:\")\n",
    "print(tabulate(best_model_table_param, headers=\"firstrow\", tablefmt=\"grid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the best model on the test set\n",
    "y_test_pred_param = best_model_instance_param.predict(X_test)\n",
    "rmse_test_param = mean_squared_error(y_test, y_test_pred_param, squared=False)\n",
    "mae_test_param = mean_absolute_error(y_test, y_test_pred_param)\n",
    "r2_test_param = r2_score(y_test, y_test_pred_param)\n",
    "\n",
    "# Print the test set metrics\n",
    "test_metrics_table_param = [\n",
    "    [\"Metric\", \"Value\"],\n",
    "    [\"Best Model\", best_model_name_param],\n",
    "    [\"RMSE\", f\"{rmse_test_param:.3f}\"],\n",
    "    [\"MAE\", f\"{mae_test_param:.3f}\"],\n",
    "    [\"R^2\", f\"{r2_test_param:.6f}\"]\n",
    "]\n",
    "print(\"\\nTest Set Metrics:\")\n",
    "print(tabulate(test_metrics_table_param, headers=\"firstrow\", tablefmt=\"grid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the predictions vs actual values for the best model after hyperparameter tuning\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.scatter(y_test, y_test_pred_param, alpha=0.6, edgecolors='w', linewidth=0.5, label='Predictions')\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], color='red', linestyle='--', linewidth=2, label='Perfect Prediction')\n",
    "plt.xlabel('Actual Values', fontsize=14)\n",
    "plt.ylabel('Predicted Values', fontsize=14)\n",
    "plt.title(f'{best_model_name_param} Predictions vs Actual Values', fontsize=16)\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.legend(fontsize=12)\n",
    "plt.tick_params(axis='both', which='major', labelsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep Learning Model\n",
    "def build_dl_model(input_dim):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(64, input_dim=input_dim, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build and train the Deep Learning Model\n",
    "input_dim = X_train.shape[1]\n",
    "dl_model = build_dl_model(input_dim)\n",
    "start_time = time.time()\n",
    "dl_model.fit(X_train, y_train, epochs=100, batch_size=32, validation_data=(X_val, y_val), verbose=2)\n",
    "end_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the deep learning model\n",
    "y_val_pred_dl = dl_model.predict(X_val)\n",
    "rmse_dl = mean_squared_error(y_val, y_val_pred_dl, squared=False)\n",
    "mae_dl = mean_absolute_error(y_val, y_val_pred_dl)\n",
    "r2_dl = r2_score(y_val, y_val_pred_dl)\n",
    "metrics['Deep Learning Model'] = {'RMSE': rmse_dl, 'MAE': mae_dl, 'R^2': r2_dl, 'Time': end_time - start_time}\n",
    "print(f'Deep Learning Model RMSE: {rmse_dl}, MAE: {mae_dl}, R^2: {r2_dl}, Time: {end_time - start_time} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the deep learning model on the test set\n",
    "y_test_pred_dl = dl_model.predict(X_test)\n",
    "rmse_dl_test = mean_squared_error(y_test, y_test_pred_dl, squared=False)\n",
    "mae_dl_test = mean_absolute_error(y_test, y_test_pred_dl)\n",
    "r2_dl_test = r2_score(y_test, y_test_pred_dl)\n",
    "print(f'Deep Learning Model Test RMSE: {rmse_dl_test}, MAE: {mae_dl_test}, R^2: {r2_dl_test}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare comparison metrics for the best base model, best hyperparameter-tuned model, and deep learning model\n",
    "\n",
    "# Evaluate the best base model on the test set\n",
    "y_test_pred_base = best_model_instance.predict(X_test)\n",
    "rmse_test_base = mean_squared_error(y_test, y_test_pred_base, squared=False)\n",
    "mae_test_base = mean_absolute_error(y_test, y_test_pred_base)\n",
    "r2_test_base = r2_score(y_test, y_test_pred_base)\n",
    "\n",
    "# Evaluate the best hyperparameter-tuned model on the test set\n",
    "y_test_pred_param = best_model_instance_param.predict(X_test)\n",
    "rmse_test_param = mean_squared_error(y_test, y_test_pred_param, squared=False)\n",
    "mae_test_param = mean_absolute_error(y_test, y_test_pred_param)\n",
    "r2_test_param = r2_score(y_test, y_test_pred_param)\n",
    "\n",
    "# Evaluate the deep learning model on the test set\n",
    "y_test_pred_dl = dl_model.predict(X_test)\n",
    "rmse_dl_test = mean_squared_error(y_test, y_test_pred_dl, squared=False)\n",
    "mae_dl_test = mean_absolute_error(y_test, y_test_pred_dl)\n",
    "r2_dl_test = r2_score(y_test, y_test_pred_dl)\n",
    "\n",
    "# Print metrics to debug\n",
    "print(\"Best Base Model Time:\", metrics[best_model_name]['Time'])\n",
    "print(\"Best Tuned Model Time:\", metrics_param[best_model_name_param]['Time'])\n",
    "print(\"Deep Learning Model Time:\", metrics['Deep Learning Model']['Time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consolidate the metrics into a dataframe for easy manipulation\n",
    "comparison_metrics = pd.DataFrame({\n",
    "    'Model': ['Best Base Model (' + best_model_name + ')', 'Best Tuned Model (' + best_model_name_param + ')', 'Deep Learning Model'],\n",
    "    'RMSE': [rmse_test_base, rmse_test_param, rmse_dl_test],\n",
    "    'MAE': [mae_test_base, mae_test_param, mae_dl_test],\n",
    "    'R^2': [r2_test_base, r2_test_param, r2_dl_test],\n",
    "    'Time': [metrics[best_model_name]['Time'], metrics_param[best_model_name_param]['Time'], metrics['Deep Learning Model']['Time']]\n",
    "})\n",
    "\n",
    "# Check the dataframe structure\n",
    "print(comparison_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the metrics using MinMax scaling\n",
    "scaler = MinMaxScaler()\n",
    "normalized_metrics = scaler.fit_transform(comparison_metrics[['RMSE', 'MAE', 'R^2']])\n",
    "normalized_df = pd.DataFrame(normalized_metrics, columns=['RMSE', 'MAE', 'R^2'], index=comparison_metrics.index)\n",
    "\n",
    "# Assign weights to each metric\n",
    "weights = {'RMSE': 0.4, 'MAE': 0.3, 'R^2': 0.3}\n",
    "\n",
    "# Compute the weighted score for each model\n",
    "normalized_df['Score'] = (\n",
    "    weights['RMSE'] * (1 - normalized_df['RMSE']) +  # Lower RMSE is better, hence (1 - RMSE)\n",
    "    weights['MAE'] * (1 - normalized_df['MAE']) +  # Lower MAE is better, hence (1 - MAE)\n",
    "    weights['R^2'] * normalized_df['R^2']  # Higher R^2 is better\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the 'Score' column to the comparison_metrics DataFrame\n",
    "comparison_metrics['Score'] = normalized_df['Score']\n",
    "\n",
    "# Find the best model based on the highest score\n",
    "best_model_idx = comparison_metrics['Score'].idxmax()\n",
    "best_model_name = comparison_metrics.loc[best_model_idx, 'Model']\n",
    "best_model_metrics = comparison_metrics.loc[best_model_idx]\n",
    "\n",
    "# Print the comparison table\n",
    "print(\"\\nComparison of Best Base Model, Best Tuned Model, and Deep Learning Model:\")\n",
    "print(tabulate(comparison_metrics, headers=\"keys\", tablefmt=\"grid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the best overall model metrics\n",
    "best_model_table = [\n",
    "    [\"Metric\", \"Value\"],\n",
    "    [\"Best Overall Model\", best_model_name],\n",
    "    [\"RMSE\", f\"{best_model_metrics['RMSE']:.3f}\"],\n",
    "    [\"MAE\", f\"{best_model_metrics['MAE']:.3f}\"],\n",
    "    [\"R^2\", f\"{best_model_metrics['R^2']:.3f}\"],\n",
    "    [\"Time (seconds)\", f\"{best_model_metrics['Time']:.4f}\"],\n",
    "    [\"Score\", f\"{best_model_metrics['Score']:.3f}\"]\n",
    "]\n",
    "print(\"\\nBest Overall Model Metrics:\")\n",
    "print(tabulate(best_model_table, headers=\"firstrow\", tablefmt=\"grid\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the best model (Random Forest in this case)\n",
    "\"\"\"\n",
    "best_rf_model = best_models['Gradient Boosting']\n",
    "with open('best_model.pkl', 'wb') as file:\n",
    "    pickle.dump(best_rf_model, file)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
